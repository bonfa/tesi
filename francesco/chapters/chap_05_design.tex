\chapter{Design}\label{chDesign}
	This chapter describes the design of the software, focusing on its main important aspects.
	The first section describes the architecture from a high-level point of view.
	The second one shows the organization of processes and threads.
	The third one explains the choices about the communication mechanism and the last one illustrates the architecture of the visual module. 
	
	\section{The Architecture}
	This section gives a high level overview of the architecture of the software by introducing its main building blocks and the main modules they interact with. 
	%Image \ref{fig:architecture} shows the building blocks of the developed components and the modules
  
	Image \ref{fig:architecture} on the left side contains the vision module, composed by the sub-modules \emph{Server} and \emph{Visual Operations}, and, on the right, \mbox{Lisp} interpreters that run \mbox{ACT-R} models.
	%On the right there are \mbox{ACT-R} models, executed by \mbox{Lisp} interpreters.
	The two parts communicate to each other thanks to a \mbox{client-server} communication mechanism based on \mbox{TCP/IP} protocol in which the \mbox{Lisp} interpreter represents the client and the visual module the server.
	
	\begin{figure}[h]
	  \begin{center} 
	    \fbox{	
	       \includegraphics[width=\textwidth*\real{0.9}]{images/ch_05/architecture.jpg}
	    }
	  \end{center} 
	  \caption{\textit{High level scheme of the architecture}}  
	  \label{fig:architecture}
 	\end{figure}


	\mbox{ACT-R} part is not discussed in this work but is fundamental for the choices about the communication protocol.
	This document focuses on the \emph{Vision Module}, whose main operations are extracting features from the input data and sending them to the models in a way that can be understood by the cognitive architecture. 
	These functions are realized by the two sub-modules contained in Vision Module.
	%The vision module does not take part in the cognitive reasoning; rather it extracts useful information that can be used by the model in its reasoning activity.

	\section{Processes}
	Figure \ref{fig:processes} represents a scheme of processes and threads that compose the architecture.
	As shown in the picture, the \mbox{Lisp} interpreter and the Visual Module run on two separated processes. 
	The processes are independent and their management is left to the operative system.

	The design of the communication protocol is such that the two processes can be run only on the same computer. 
	The images to be processed, in fact, at the moment are not sent as messages from the client to the server but are stored on the computer which runs both the processes.
	Anyway, it is enough to modify the message exchange by adding a message sent from the client to the server encapsulating the image data in order to run them on two different computers.
	
	\begin{figure}[h]
	  \begin{center} 
	    \fbox{	
	       \includegraphics[width=\textwidth*\real{0.9}]{images/ch_05/processes.jpg}
	    }
	  \end{center} 
	  \caption{\textit{Scheme of processes and threads of the architecture}}  
	  \label{fig:processes}
 	\end{figure}

	
	The Vision Module is composed by two threads: one for the server and the other for the visual processing. 
	The server thread runs in background while the visual operation one is the main thread.
	The problems regarding \mbox{race conditions} are dealt with \emph{semaphores}; all the other issues related to threads management are automatically handled by the \emph{Boost\footnote{Boost is a set of libraries for C++ that support many issues including multi-threading. For more information see the official website at \url{www.boost.org}}} library. 
	The thread organization of \mbox{ACT-R} is not discussed in this work.	


	\section{Communication}
	In order to exchange information between each other, \mbox{ACT-R} and the developed module use a synchronous \emph{client-server} communication mechanism based on \mbox{TCP}. 
	\mbox{ACT-R} models represent the clients, the developed software the server. 
	The format used for the messages is \mbox{JSON}.  		

	The client-server approach has been chosen because it is standardized and supported by the interpreters that run \mbox{ACT-R}. 
	Moreover, the time spent for coding and decoding the message is negligible if compared to the time needed for the image processing.
	Finally, the possibility of using shared resources and the transparency offered by the mechanism ensure very high flexibility and scalability.
	

\begin{comment}
	The client-server approach has been chosen because it is standardized and is supported by the interpreters that run \mbox{ACT-R}. 
	In addition, the possibility of using shared resources and the transparency offered by the mechanism ensure very high flexibility and scalability.
	The con of this method is the overhead introduced for the inter process communication. In fact, it needs an intermediate format of messages and instruments for coding and decoding data.
	In this case, anyway, the time needed to perform these operations is negligible if compared to the time needed for the image processing.
\end{comment}

	Another solution analyzed for the communication between \mbox{ACT-R} and the Visual Module is compiling the Visual Module as a static library and then loading it directly in the \mbox{Lisp} interpreter.
	The latter is able to call the static functions thanks to \emph{foreign functions}.
	%The calls to the static library are then called thanks to a wrapper and 
	In this way, the communication is more efficient because all the overhead introduced due to the inter process communication is avoided. 
	This method, though, is not maintainable. 
	In fact, every \mbox{Lisp} interpreter has a different method of calling foreign functions~\cite{SWIGDoc}. 
	As consequence of this, every time the interpreter is changed, the interface of the visual module must be re-written.
 	At design time it has been decided to prefer maintainability to an irrelevant improvement of the performances, thus the client-server solution has been chosen.

	TCP has been preferred to \mbox{UDP} mainly because of the reliability guaranteed by that protocol. 
	Secondly, the bi-directionality of the communication and the small quantity of data exchanged make it more suitable for the purpose.

	The communication is synchronous. 
	The client, once having sent the request, stays pending until it receives the response. 
	This choice is coherent with the application domain: as the software module acts like a vision system for the cognitive architecture, it makes sense that the model does not continue the reasoning until the available visual information is received.

	The \mbox{Lisp} interpreter which runs \mbox{ACT-R} models is the client, while the developed software is the server.  	
	When a model needs to acquire information about an image, it connects to the server. 
	Once connected, it can encode and send a request to the server. 
	After the server has received and decoded successfully the request, it processes the input data. 
	When the visual information is ready to be sent, the server encodes it and send it back to the client. 
	After this, the message is decoded and its content is translated in chunks and then delivered to the model, that uses it for the cognitive reasoning.

	All the messages exchanged between client and server are encoded in \mbox{JSON}. 
	Appendix \ref{appB} contains an example of these messages.
%		In the next paragraph will be described more in detail how the data are encoded.


	\section{Vision Operation Module}
	This section describes the design of the Vision Operation Module.
	For reasons of clarity, the description of the architecture follows a top-down approach and makes use of diagrams. 		The first subsection gives an overview of the module and the following ones study in deep specific aspects of it.

	To better explain the contents and the structure of the design, the description is supported by \mbox{UML} diagrams. 
	The following sections describe the module at different levels of detail and the diagrams reflect this approach.
	In subsection \ref{visOpModArch}, for example, they are used only to present how the classes are organized in the overall structure and that is why they contain only the names of the classes. 
	Further details are added in the diagrams in the next subsections, according with the top-down approach of the chapter.
	
	The reader will notice that some classes that appear in the following diagrams are not described in the chapter. The reason of this is that they do not accomplish to any functions described in the requirements section of \mbox{chapter \ref{chObjective}}. Their presence makes sense in the context of the whole software, described in \ref{TeamGoal}. It has been decided not to remove such classes from the diagrams because they influence the architecture and motivate particular design and implementation choices.

	This section explains only the most important architectural aspects of the software. 
	Not particularly interesting classes, like, for example, the utility ones are not described here.
	For a more detailed description about all the aspects of the architecture, see appendix \ref{appA}.

		\subsection{Architecture}\label{visOpModArch}
		The vision operation module is composed of three main parts, one for processing images and videos, one for containing the hierarchy of the objects recognized in the recognition process and one containing the exception classes and the utility functions used by all the other components.

	\begin{comment}
		The vision module is composed of four main parts, one for processing images and videos, one for containing the hierarchy of the objects recognized during such processing, one for the communication with the model and one containing all the utility functions used by all the other components.
	\end{comment}	

		%\todo{change picture}
		\begin{figure}[h]
		  \begin{center} 
		    \fbox{	
		       \includegraphics[scale=0.55]{images/ch_05/progettazione_overview.png}
		    }
		  \end{center} 
		  \caption{\textit{Class diagram illustrating a high level overview of the vision operation module}}  
		  \label{fig:classOverview}
	 	\end{figure}

		%\todo{change this part}
		Image \ref{fig:classOverview} shows a class diagram which describes two of the parts that compose the software. 
		The part dedicated to the processing of images and videos is located on the left and is composed by the classes \emph{Proxy}, \emph{FeatureGetter}, \emph{FeatureExtractor}, \emph{Input}, \emph{MarkerDetector} and \emph{QRScanner}.
		The object hierarchy, which is shown on the right, contains the classes \emph{Object}, \emph{BoundingBox}, \emph{Blob}, \emph{QR}, \emph{Circle}, \emph{Triangle}, \emph{Quadrilateral}, \emph{Button} and \emph{Marker}.

		%\todo{change this part}
		The third part is not included in the section because it contains only utility classes and functions. 
		The reader can find a description of that part in appendix \ref{appA}. 


		\subsection{Feature Extraction Classes}\label{featExtraction}	
		The classes that play the role of acquiring the images and extracting features from them are \emph{Input}, \emph{FeatureGetter} and \emph{FeatureExtractor}. \emph{Proxy} has the role of intermediation between this part of the software and the one which manages the communication with \mbox{ACT-R}.

		\begin{figure}[h]
		  \begin{center} 
		    \fbox{	
		       \includegraphics[scale=0.65]{images/ch_05/feature.png}
		    }
		  \end{center} 
		  \caption{\textit{Class diagram illustrating the main roles in the feature extraction process}}  
		  \label{fig:FeatureDesign}
	 	\end{figure}
	
		Figure \ref{fig:FeatureDesign} contains the class diagram that designs the interface of these classes.
		\emph{FeatureExtractor} represents the core of this part. It receives as input an image and processes it in order to extract the features it contains. For each extracted feature it creates a correspondent element in the \emph{object hierarchy}. Such hierarchy is described in section \ref{classHierarchy}. Once the objects are generated, the class creates a list containing them and delivers it to FeatureGetter. 
		FeatureExtractor is also responsible for accomplishing other goals described in the requirements, like recognizing colors of pixels and objects, calculating the distances of two objects as well as the relative position of one object in respect with another one and making dimensional comparisons between two objects. These features have not been included in the initial design of the class. %even if it has been thought that this class would have accomplished all these tasks.

		\emph{Input} is responsible for interacting with the file system and the hardware of the computer. In particular its tasks are loading images from files, saving images to files and loading video streams from the web-cam. The acquired data are delivered to FeatureGetter. 

		\emph{FeatureGetter} represents a level of abstraction that avoids to call directly the functions of FeatureExtractor and Input, providing a simpler interface. Its functions create the Input object, receive from it the input data, delivers it to FeatureExtractor for the extraction of the features and delivers the received object list to Proxy.

		\emph{Proxy} represents a further level of abstraction between FeatureGetter and the part of the software which manages the communication with ACT-R. Most of the procedures for the communication with the cognitive architecture, in fact, need to use specific data types. The main function of this class is to transform each object of the object list in the type necessary for the communication to ACT-R.

		As the classes \emph{MarkerDetector} and \emph{QRScanner} do not accomplish to any functions described in the requirements section of chapter \ref{chObjective}, they are not described in this work.

		

		\section{Class Hierarchy}\label{classHierarchy}
		The class hierarchy contains all the objects that are recognized during the processing of the image. Each feature has a correspondent object that describes it. 
	
		\begin{figure}[h]
		  \begin{center} 
		    \fbox{	
		       \includegraphics[scale=0.65]{images/ch_05/hierarchy.png}
		    }
		  \end{center} 
		  \caption{\textit{Class diagram illustrating the class hierarchy of the recognized objects}}  
		  \label{fig:HierarchyDesign}
	 	\end{figure}

		Figure \ref{fig:HierarchyDesign} contains a class diagrams that specifies such hierarchy. 
		\emph{Object} is the superclass that represents all the possible objects recognized in an image during the feature extraction process. 
		All the elements of this type must have two attributes: a \emph{rotation} value and a \emph{boundingBox}. 
		The latter is defined by the class \emph{BoundingBox}. Such element can have only rectangular shape, in fact its attributes are the coordinates of a point and horizontal and vertical dimensions.


		The type Object can be specified by two sub-types: \emph{Blob}, that represents a generic shape, and \emph{QR}, representing a \mbox{QR} element. The presence of \mbox{QR} is not important for the shape recognition task but it justifies the presence of Blob in the hierarchy. Without \mbox{QR} class, in fact, Blob could have been incorporated in Object.
		Blob is provided of the attributes \emph{area} and \emph{perimeter}, which, as suggested by the names, represent respectively the area and the perimeter of the shape.
	

		The classes \emph{Circle}, \emph{Triangle} and \emph{Quadrilateral} extend \emph{Blob}. They represent the simple shapes that can be recognized during the recognition process. Each item is defined starting by basic geometrical elements: the circle by center and radius, the triangle by three points, the quadrilateral by four points. \emph{Point} class, in particular, has been introduced in this diagram for reasons of completeness. 

		The class \emph{Button} specifies Quadrilateral by adding the attribute \emph{text}, which represents the message of such button. %The fact that such class derives Quadrilateral instead of Blob reflects the requirement that the buttons can be only rectangular. 
		The class \emph{Marker} is not described in this document. 
	
