\chapter{State Of The Art}
  %% ACT-R
  \subsection{ACT-R}
	ACT-R, acronym of Adaptive Control of Thought-Rational, is a cognitive architecture: a theory about how human cognition works. 
	The aim of\mbox{ACT-R} is to model the behaviour of the human brain. From an external point of view\mbox{ACT-R} can be intended as a programming language; however its internal structure is based on assumptions about human cognition. The assumptions are established by psychologists, who research and study the human cognition, through numerous experiments ~\cite{Allen94}. 

	A feature that distinguishes\mbox{ACT-R} from other theories in the field relies in allowing researchers to collect quantitative measures that can be directly compared with the quantitative measures obtained from human participants. These measures are compared between humans and\mbox{ACT-R}, they are the traditional one of cognitive psychology: time needed to perform the task, accuracy in the task and neurological data. For each task the researchers write programs in\mbox{ACT-R}, called models, adding their own assumptions related to the task. A model simulates the behaviour of a cognitive agent. The approach based on measuring and comparison leads to very effective models, in example it had been shown that\mbox{ACT-R} can successfully predict \emph{Blood-Oxygen-Level Dependence (BOLD)} of several parts of the brain, through experiments with \emph{Functional Magnetic Resonance Imaging (fMRI)}.
	Up to now\mbox{ACT-R} has been used successfully to create models in many different areas, the most important of witch are: learning, problem solving, communication and perception.

	ACT-R's most important assumption is that human knowledge can be divided into two irreducible kinds of representations: declarative and procedural.
	Declarative memory refers to all the kind of memory that can be consciously recalled, like the shape of an object or its color. Procedural memory is, instead, very different from the declarative one: it is referred to actions that can be done unconsciously. The most common way, used to explain how these two kind of memory work, is the typewriter example.
	A skilled typewriter can write quickly without looking at the keyboard, he just puts his fingers in the right place and push the right keys, in the correct order. If we ask to the same typewriter where a certain character is positioned in his keyboard, he will probably answer that he's not sure about that without looking at it, even if he writes the whole day. In this example the procedural memory is stored as the action of putting the right finger in the right place when the typewriter has the goal to write a certain character.

	In\mbox{ACT-R} theory the declarative memory is represented by the so called \emph{chunks}, data structures defined as a n-tuple (slots) of arbitrary size. The n-tuples and their values are specified in the \emph{chunk-type}, it defines also (if any) the initial value of the slot. Every chunk has also a name, used to reference it, but that is not considered to be a part of the chunk itself. The chunks and their slots are used to implement the mechanisms of\mbox{ACT-R} theory, even if the slots are only software construct and not part of the theory itself. The chunk-types can be organized in hierarchy.

	The\mbox{ACT-R} counter part of the human procedural memory are the \emph{productions}. Productions are the\mbox{ACT-R} equivalent of functions, they define sequences of actions and can be fired only if a set of preconditions are satisfied. 

	Both the declarative memory and the procedural memory are called \emph{modules} of \mbox{ACT-R}.  The many activities carried out by the brain, i.e. talking or moving, are performed by neurons located together, in the same areas. In analogue way each module of\mbox{ACT-R} represents a function of the human brain. In all this infrastructure the task of the procedural module is to coordinate all the other modules. The exchange of information between the modules is achieved through \emph{buffers}.

	The buffers are the built in interface between modules. Every buffer is connected to a module, while a module can also have no buffers. The role of the modules is to relay queries for actions from the module and to get information about the internal state of the module. Each module can hold maximum one chunk, accessible in read only from all the other module, and in read and write mode for the module owner of the buffer. The modules usually work in a parallel way, but their interaction is only serial. There are two reasons for this kind of interaction: the first one is due to the structure of the buffers, that can hold only one chunk at a certain time; the second one is to allow only one production, which have its preconditions satisfied, to be fired. When the preconditions of more than one production are satisfied the one to be fired is the one with the higher \emph{utility value}. This is a numeric quantity, it can be associated with each production in advance or it can be learned while the model runs.

	From an informatics point of view ACT-R is a software written in Lisp, and its functionalities are offered in a Lisp-like language. It is taught to have a modular structure so that it can be easily extended to fit to different tasks. ACT-R is the ultimate successor of a series of increasingly precise models of human cognition, developed by John R. Anderson. Anderson credits Allen Newell as source of influence to his theory.




  %% OPENCV
  \subsection{OpenCV}
	OpenCV, an abbreviation that stands for \emph{Open Source Computer Vision}, is a computer vision library that was originally developed by Intel and, later on, by Willow Garage.
	It is a cross-platform library, released under a BSD license, thus it is free and open source. In the beginning it was developed in C and C++ and afterwards it was expanded by the addition of interfaces for Java and Python. OpenCV is designed for computational efficiency and with a strong focus on real-time applications. The version 2.4 has more than 2500 algorithms. The library has been used in many applications as, for example, mine inspection and robotics \cite{OpenCV:MainWebPage}. The following sections contain a brief history of the library and a list of its main features.
		
	\subsubsection*{History}
	The OpenCV Project started in 1999 as an Intel Reasearch initiative aimed to improve CPU intensive applications as a part of projects including real-time ray tracing and 3D display walls. The early goals of the project were developing optimized code for basic vision infrastructure, spreading this infrastructure to developers and making it portable and available for free, using a license that let the developers create both commercial and free applications.\newline
	The first alpha version was released to the public in 2000, followed by five beta versions between 2001 and 2005, which lead to version 1.0 in 2006. In 2008, the technology incubator Willow Garage begun supporting the project and, in the same year, version 1.1  was released.
	In October 2009, OpenCV 2.0 was released. It includes many improvements, such as a better C++ interface, more programming patterns, new functions and an optimization for multi-core architectures. According to the current OpenCV release plan, a new version of the library is delivered on a six-months basis. \cite{OpenCV:ChangeLogs}.
	
	\subsubsection*{Main Features}
	OpenCV offers a wide range of possibilities. First of all, it provides an easy way to manage image and video data types. It also offers functions to load, copy, edit, convert and store images and a basic graphical user interface that lets the developers handle keyboard and mouse and display images and videos. The library lets manipulate images even with matrix and vector algebra routines. It supports the most common dynamic data structures and offers many different basic image processing functions: filtering, edge and corner detection, color conversion, sampling and interpolation, morphological operations, histograms and image pyramids. Beyond this, it integrates many functions for structural analysis of the image, camera calibration, motion analysis and object recognition. \cite{Agam2006}.
